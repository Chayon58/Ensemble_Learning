{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chayon58/Ensemble_Learning/blob/main/Ensember_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETSIgxQWhVDm",
        "outputId": "786a5c53-5369-4c6f-c44b-8214f9ab6b80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zKdAGU41k8fS",
        "outputId": "fe56902a-032c-4f1a-9841-181fb87f139e"
      },
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 0: Install dependencies\n",
        "# ========================================\n",
        "!pip install ultralytics scikit-learn albumentations opencv-python-headless matplotlib\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ========================================\n",
        "# STEP 1: Dataset Loader\n",
        "# Supports jpg, jpeg, png\n",
        "# ========================================\n",
        "class LabeledImageDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.labels = pd.read_csv(csv_file)  # columns: filename,label\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.labels.iloc[idx, 0]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        label = int(self.labels.iloc[idx, 1])\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "        return image, label\n",
        "\n",
        "class UnlabeledImageDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        self.imgs = [f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.imgs[idx])\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)[\"image\"]\n",
        "        return image, self.imgs[idx]\n",
        "\n",
        "# ========================================\n",
        "# STEP 2: User Config\n",
        "# ========================================\n",
        "img_dir = \"/content/drive/MyDrive/images\"   # folder with user-submitted images\n",
        "csv_labels = None  # set CSV path if you have labels\n",
        "\n",
        "aug_transform = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.2),\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "    A.Normalize(),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "if csv_labels:\n",
        "    dataset = LabeledImageDataset(csv_labels, img_dir, transform=aug_transform)\n",
        "    labeled = True\n",
        "else:\n",
        "    dataset = UnlabeledImageDataset(img_dir, transform=aug_transform)\n",
        "    labeled = False\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "print(\"Number of images:\", len(dataset))\n",
        "\n",
        "# ========================================\n",
        "# STEP 3: Models\n",
        "# ========================================\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(224*224*3, latent_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 224*224*3),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Unflatten(1,(3,224,224))\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        out = self.decoder(z)\n",
        "        return out, z\n",
        "\n",
        "autoencoder = AutoEncoder().to(device)\n",
        "\n",
        "if labeled:\n",
        "    cnn = models.resnet18(pretrained=True)\n",
        "    num_classes = len(set(pd.read_csv(csv_labels)['label']))\n",
        "    cnn.fc = nn.Linear(cnn.fc.in_features, num_classes)\n",
        "    cnn = cnn.to(device)\n",
        "\n",
        "yolo = YOLO(\"yolov8n.pt\")  # pretrained YOLOv8\n",
        "\n",
        "# ========================================\n",
        "# STEP 4: Train Autoencoder\n",
        "# ========================================\n",
        "print(\"Training Autoencoder...\")\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(2):\n",
        "    autoencoder.train()\n",
        "    for batch in dataloader:\n",
        "        imgs = batch[0] if labeled else batch[0]\n",
        "        imgs = imgs.to(device)\n",
        "        outputs, _ = autoencoder(imgs)\n",
        "        loss = criterion(outputs, imgs)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"[Autoencoder] Epoch {epoch+1}, Loss={loss.item():.4f}\")\n",
        "\n",
        "# ========================================\n",
        "# STEP 5: Train CNN (if labeled)\n",
        "# ========================================\n",
        "if labeled:\n",
        "    print(\"Training CNN...\")\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(cnn.parameters(), lr=1e-4)\n",
        "    for epoch in range(2):\n",
        "        cnn.train()\n",
        "        for imgs, labels in dataloader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = cnn(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"[CNN] Epoch {epoch+1}, Loss={loss.item():.4f}\")\n",
        "\n",
        "# ========================================\n",
        "# STEP 6: YOLO Detection, Bounding Boxes, Masking\n",
        "# ========================================\n",
        "detection_confidences = []\n",
        "masked_crops_all = []\n",
        "\n",
        "for idx in range(len(dataset)):\n",
        "    img_data, img_name = dataset[idx] if not labeled else dataset[idx]\n",
        "\n",
        "    np_img = img_data.permute(1,2,0).cpu().numpy() if torch.is_tensor(img_data) else img_data\n",
        "    np_img_vis = (np_img * 255).astype(np.uint8).copy()\n",
        "\n",
        "    results = yolo.predict(np_img, verbose=False)\n",
        "\n",
        "    crops = []\n",
        "    confidences = []\n",
        "\n",
        "    if results[0].boxes is not None and len(results[0].boxes) > 0:\n",
        "        for i, box in enumerate(results[0].boxes.xyxy):\n",
        "            x1, y1, x2, y2 = box.cpu().numpy().astype(int)\n",
        "            conf = results[0].boxes.conf[i].cpu().numpy()\n",
        "            crop = np_img[y1:y2, x1:x2]\n",
        "            crop = cv2.resize(crop, (224,224))\n",
        "            crops.append(crop)\n",
        "            confidences.append(conf)\n",
        "\n",
        "            # Draw bounding box\n",
        "            cv2.rectangle(np_img_vis, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "            cv2.putText(np_img_vis, f\"{conf:.2f}\", (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
        "    else:\n",
        "        crop = cv2.resize(np_img, (224,224))\n",
        "        crops.append(crop)\n",
        "        confidences.append(0)\n",
        "\n",
        "    masked_crops_all.append([torch.tensor(c).permute(2,0,1).float()/255.0 for c in crops])\n",
        "    detection_confidences.append(np.mean(confidences))\n",
        "\n",
        "    # Show image with bounding boxes\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(np_img_vis)\n",
        "    plt.title(f\"{img_name} - {len(crops)} objects detected\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# ========================================\n",
        "# STEP 7: Detection Confidence Histogram\n",
        "# ========================================\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(detection_confidences, bins=10, color='skyblue', edgecolor='black')\n",
        "plt.xlabel(\"Detection Confidence\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.title(\"Histogram of YOLO Detection Confidence per Image\")\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# STEP 8: Feature Extraction for Ensemble\n",
        "# ========================================\n",
        "def extract_auto(model, dataloader):\n",
        "    model.eval()\n",
        "    feats, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            if labeled:\n",
        "                imgs, lbls = batch\n",
        "            else:\n",
        "                imgs, lbls = batch[0], None\n",
        "            imgs = imgs.to(device)\n",
        "            _, z = model(imgs)\n",
        "            feats.append(z.cpu().numpy())\n",
        "            if labeled: labels.append(lbls.numpy())\n",
        "    return np.vstack(feats), (np.hstack(labels) if labeled else None)\n",
        "\n",
        "def extract_cnn(model, dataloader):\n",
        "    feats, labels = [], []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in dataloader:\n",
        "            imgs = imgs.to(device)\n",
        "            out = model(imgs)\n",
        "            probs = torch.softmax(out, dim=1).cpu().numpy()\n",
        "            feats.append(probs)\n",
        "            labels.append(lbls.numpy())\n",
        "    return np.vstack(feats), np.hstack(labels)\n",
        "\n",
        "print(\"Extracting features...\")\n",
        "auto_feats, y = extract_auto(autoencoder, dataloader)\n",
        "cnn_feats, _ = (extract_cnn(cnn, dataloader) if labeled else (None,None))\n",
        "yolo_feats = np.vstack([torch.stack(crops).view(len(crops),-1).numpy() for crops in masked_crops_all])\n",
        "\n",
        "# ========================================\n",
        "# STEP 9: Ensemble + Metrics\n",
        "# ========================================\n",
        "if labeled:\n",
        "    print(\"Training meta-classifier...\")\n",
        "    X_parts = [auto_feats, yolo_feats]\n",
        "    if cnn_feats is not None:\n",
        "        X_parts.append(cnn_feats)\n",
        "    X = np.hstack(X_parts)\n",
        "\n",
        "    meta = LogisticRegression(max_iter=1000)\n",
        "    meta.fit(X, y)\n",
        "    preds = meta.predict(X)\n",
        "\n",
        "    acc = accuracy_score(y, preds)\n",
        "    prec, recall, f1, _ = precision_recall_fscore_support(y, preds, average=\"weighted\")\n",
        "    print(f\"✅ Ensemble Results: Acc={acc:.4f}, Precision={prec:.4f}, Recall={recall:.4f}, F1={f1:.4f}\")\n",
        "else:\n",
        "    print(\"✅ Unsupervised run complete (Autoencoder + YOLO features extracted).\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfjWlJT2Op5+hCgpLGACgg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}